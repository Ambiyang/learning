# P14
1. Bad performance of training error does not always mean over-fitting you shall decide it by comparing with test error.  
In a case,56-layer does worse than 20-layer on training set which remind you of over-fitting but indeed 56-layer's testing error is nearly the same   
while 20-layers has a much worse value than training set.So,it's a result of bad training methos(you shall try other lr or actiavation function)
2.  Sigmoid time there was no deep model because of vanish decendent.ReLU fixs this problem but its not derivable at 0 where you can just give a random value,  
nowaday models use max out which enables the actiavation function being learnable.PS.relu,maxout also has a same effect as drop-out.
3.  Adam = RMSProp + Momentum
4.  L2 regularization decays big paras while L1 decays small paras quickly.
5.  drop-out is like ensemble.the final weights must be adjusted by multiplying (1-p)%  
