# P5
1.  AdaGrad:
2.  Scaling: gradient shape is a circle,every dim can share the same lr.

# P9
1.  Adam need warm-up?

# P11
1.  Logistic regression use sigmoid func and loss fun is cross-entrophy
2.  Generative VS Discriminative model:Generative model suppose a probability model is better when data is few.  

# P14
1. Bad performance of training error does not always mean over-fitting you shall decide it by comparing with test error.  
In a case,56-layer does worse than 20-layer on training set which remind you of over-fitting but indeed 56-layer's testing error is nearly the same   
while 20-layers has a much worse value than training set.So,it's a result of bad training methos(you shall try other lr or actiavation function)
2.  Sigmoid time there was no deep model because of vanish decendent.ReLU fixs this problem but its not derivable at 0 where you can just give a random value,  
nowaday models use max out which enables the actiavation function being learnable.PS.relu,maxout also has a same effect as drop-out.
3.  Adam = RMSProp + Momentum
4.  L2 regularization decays big paras while L1 decays small paras quickly.
5.  drop-out is like ensemble.the final weights must be adjusted by multiplying (1-p)%  

# P15
1.  one layer can represent any continuous function however,deep structure is more effective.
2.  deep model can classify similar features.

# P21
1.  RNN:gradient vanish or explode,LSTM can handle gradient vanish but not with explode
 
# P22
1.  High density similarity:cluster and then label
2.  Radial Basis Function(RBF):calc similarity

# P44
1.  bigger nw is easier to train.training is like lottery.
2.  prune:neuron is better than weight.
